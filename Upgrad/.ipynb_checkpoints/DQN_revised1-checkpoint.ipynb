{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10 # number of locations, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 30 # number of days, ranges from 0 ... d-1\n",
    "episode_length = 1000\n",
    "fixed_act_Set = 15\n",
    "epsilon = 0.99\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "gamma = 0.95\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "Time_matrix = np.random.randint(1, 11,(m, m))\n",
    "actions = []\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        actions.append([i,j])\n",
    "states = []\n",
    "for i in range(m):\n",
    "    for j in range(t):\n",
    "        for k in range(d):\n",
    "            states.append([i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_trans(state,action,m,t,d):\n",
    "    state_encod = np.zeros(m+t+d+m+m)\n",
    "    state_encod.reshape(1,84)\n",
    "    state_encod[current_state[0]] = 1\n",
    "    state_encod[m+current_state[1]] = 1\n",
    "    state_encod[m+t+current_state[2]] = 1\n",
    "    state_encod[m+t+d+action[0]] = 1\n",
    "    state_encod[m+t+d+m+action[1]] = 1\n",
    "    return state_encod   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(Q_value,epsilon,actions):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        index = np.random.randint(0, m*m,(1, 1))\n",
    "        if actions[np.int(index)][0] == actions[np.int(index)][1] and actions[np.int(index)][1] != 0:\n",
    "            return act(Q_value,epsilon,actions)\n",
    "        else:\n",
    "            return index\n",
    "    else:\n",
    "        index = np.argmax(Q_value)\n",
    "        if actions[np.int(index)][0] == actions[np.int(index)][1] and actions[np.int(index)][1] != 0:\n",
    "            return act(Q_value,epsilon,actions)\n",
    "        else:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(state, action, time_matrix):\n",
    "    start_loc, time, day = state\n",
    "    pickup, drop = action\n",
    "    if pickup == 0 and drop == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return time_matrix[pickup, drop] - time_matrix[start_loc, pickup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state_func(state, action, time_matrix, t, d):\n",
    "    start_loc, time, day = state\n",
    "    pickup, drop = action\n",
    "    if pickup == 0 and drop == 0:\n",
    "        time_elapsed = 1\n",
    "        drop = start_loc\n",
    "    else:\n",
    "        time_elapsed = time_matrix[start_loc, pickup] + time_matrix[pickup, drop]\n",
    "    time_next = (time + time_elapsed) % t\n",
    "    day_next = (day + (time + time_elapsed)//t) % d\n",
    "    return drop, time_next, day_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_DQN(m,t,d,actions,X,index_State):\n",
    "    state_pred = X[0:m+t+d]\n",
    "    X_test = np.zeros((len(actions), t+d+3*m))\n",
    "    for i in range(len(actions)):\n",
    "        action_encod_p = np.zeros(m+m)\n",
    "        action_encod_p[actions[i][0]] = 1\n",
    "        action_encod_p[m + actions[i][1]] = 1\n",
    "        dummy = np.hstack((state_pred,action_encod_p))\n",
    "        X_test[i,:] = dummy\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction = prediction.reshape(len(actions))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7269607]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = np.int(m+t+d+m+m),activation ='relu'))\n",
    "model.add(Dense(100,activation ='relu'))\n",
    "model.add(Dense(1,activation ='linear'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:2,Episode:0, Epsilon:0.99\n",
      "Reward:-1,Episode:50, Epsilon:0.98505\n",
      "Reward:-1,Episode:100, Epsilon:0.7666767843404657\n",
      "Reward:5,Episode:150, Epsilon:0.5967141684651915\n",
      "Reward:-1,Episode:200, Epsilon:0.46443013029723174\n",
      "Reward:5,Episode:250, Epsilon:0.36147180229136094\n",
      "Reward:5,Episode:300, Epsilon:0.28133804274959967\n",
      "Reward:0,Episode:350, Epsilon:0.21896893145312793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-1775fedd3188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mindex_State\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_DQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_State\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTime_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTime_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-40403716933f>\u001b[0m in \u001b[0;36mprediction_DQN\u001b[0;34m(m, t, d, actions, X, index_State)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction_encod_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_encod_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# code for genrating episode\n",
    "memory = deque(maxlen=2000)\n",
    "current_state =  (np.random.randint(0, m),np.random.randint(0, t),np.random.randint(0, d))\n",
    "count  =  0\n",
    "for epi_len in range(episode_length):\n",
    "    # pick a random action\n",
    "    count = count + 1\n",
    "    index_State = states.index([current_state[0],current_state[1],current_state[2]])\n",
    "    X = state_trans(current_state,[0,0],m,t,d)\n",
    "    action = actions[np.int(act(prediction_DQN(m,t,d,actions,X,index_State),epsilon,actions))]\n",
    "    reward = reward_func(current_state, action, Time_matrix)\n",
    "    next_state = next_state_func(current_state, action, Time_matrix, t, d)\n",
    "    X = state_trans(current_state,action,m,t,d)\n",
    "    #print(\"State: \",current_state, \" Action: \", action, \" Reward: \", reward, \" Nextstate: \", next_state)\n",
    "    index_Next_State = states.index([next_state[0],next_state[1],next_state[2]])\n",
    "    memory.append((X,reward,index_Next_State))\n",
    "    current_state = next_state\n",
    "    if count > batch_size:\n",
    "        mini_batch = random.sample(memory, batch_size)\n",
    "        target_mini_batch = np.zeros((batch_size, 1))\n",
    "        input_mini_batch = np.zeros((batch_size, t+d+3*m))\n",
    "        for i in range(batch_size):\n",
    "            X, reward, index_Next_state = mini_batch[i]\n",
    "            target = reward + gamma*np.argmax(prediction_DQN(m,t,d,actions,X,index_Next_State))\n",
    "            input_mini_batch[i, :] = X\n",
    "            target_mini_batch[i, :] = target\n",
    "        model.fit(input_mini_batch,target_mini_batch, epochs = 1, verbose = 0)\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "    if epi_len % 50 == 0:\n",
    "        print(\"Reward:{},Episode:{}, Epsilon:{}\".format(reward,epi_len,epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
