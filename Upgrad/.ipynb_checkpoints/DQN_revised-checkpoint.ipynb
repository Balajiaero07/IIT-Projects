{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10 # number of locations, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 30 # number of days, ranges from 0 ... d-1\n",
    "episode_length = 10000\n",
    "fixed_act_Set = 15\n",
    "epsilon = 0.99\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "gamma = 0.95\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "Time_matrix = np.random.randint(1, 11,(m, m))\n",
    "Q_value = np.random.randint(-1, 0,(m*t*d, m*m))\n",
    "actions = []\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        actions.append([i,j])\n",
    "states = []\n",
    "for i in range(m):\n",
    "    for j in range(t):\n",
    "        for k in range(d):\n",
    "            states.append([i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_trans(state,action,m,t,d):\n",
    "    state_encod = np.zeros(m+t+d+m+m)\n",
    "    state_encod.reshape(1,84)\n",
    "    state_encod[current_state[0]] = 1\n",
    "    state_encod[m+current_state[1]] = 1\n",
    "    state_encod[m+t+current_state[2]] = 1\n",
    "    state_encod[m+t+d+action[0]] = 1\n",
    "    state_encod[m+t+d+m+action[1]] = 1\n",
    "    return state_encod   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state,Q_value,epsilon,actions):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        index = np.random.randint(0, m*m,(1, 1))\n",
    "        if actions[np.int(index)][0] == actions[np.int(index)][1]:\n",
    "            return act(state,Q_value,epsilon,actions)\n",
    "        else:\n",
    "            return index\n",
    "    else:\n",
    "        index = np.argmax(Q_value[state])\n",
    "        if actions[np.int(index)][0] == actions[np.int(index)][1]:\n",
    "            return act(state,Q_value,epsilon,actions)\n",
    "        else:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(state, action, time_matrix):\n",
    "    start_loc, time, day = state\n",
    "    pickup, drop = action\n",
    "    if pickup == 0 and drop == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return time_matrix[pickup, drop] - time_matrix[start_loc, pickup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state_func(state, action, time_matrix, t, d):\n",
    "    start_loc, time, day = state\n",
    "    pickup, drop = action\n",
    "    if pickup == 0 and drop == 0:\n",
    "        time_elapsed = 1\n",
    "        drop = start_loc\n",
    "    else:\n",
    "        time_elapsed = time_matrix[start_loc, pickup] + time_matrix[pickup, drop]\n",
    "    time_next = (time + time_elapsed) % t\n",
    "    day_next = (day + (time + time_elapsed)//t) % d\n",
    "    return drop, time_next, day_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_DQN(m,t,d,actions,X,index_State, Q_value):\n",
    "    state_pred = X[0][0:m+t+d]\n",
    "    for i in range(len(actions)):\n",
    "        action_encod_p = np.zeros(m+m)\n",
    "        action_encod_p[actions[i][0]] = 1\n",
    "        action_encod_p[m + actions[i][1]] = 1\n",
    "        X_test = np.hstack((state_pred,action_encod_p))\n",
    "        X_test = X_test.reshape(1,t+d+3*m)\n",
    "        Q_value[index_State][i] = model.predict(X_test)\n",
    "    return Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = np.int(m+t+d+m+m),activation ='relu'))\n",
    "model.add(Dense(100,activation ='relu'))\n",
    "model.add(Dense(1,activation ='linear'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:2,Episode:0, Epsilon:0.37064574667155564\n",
      "Reward:1,Episode:50, Epsilon:0.36879251793819784\n",
      "Reward:1,Episode:100, Epsilon:0.28703584766426177\n"
     ]
    }
   ],
   "source": [
    "# code for genrating episode\n",
    "memory = deque(maxlen=2000)\n",
    "current_state =  (np.random.randint(0, m),np.random.randint(0, t),np.random.randint(0, d))\n",
    "count  =  0\n",
    "for epi_len in range(episode_length):\n",
    "    # pick a random action\n",
    "    count = count + 1\n",
    "    index_State = states.index([current_state[0],current_state[1],current_state[2]])\n",
    "    action = actions[np.int(act(index_State,Q_value,epsilon,actions))]\n",
    "    reward = reward_func(current_state, action, Time_matrix)\n",
    "    next_state = next_state_func(current_state, action, Time_matrix, t, d)\n",
    "    #print(\"State: \",current_state, \" Action: \", action, \" Reward: \", reward, \" Nextstate: \", next_state)\n",
    "    X = state_trans(current_state,action,m,t,d)\n",
    "    memory.append((X,reward,index_State))\n",
    "    current_state = next_state\n",
    "    if count > batch_size:\n",
    "        mini_batch = random.sample(memory, batch_size)\n",
    "        for X, reward, index_State in mini_batch:\n",
    "            target = reward + gamma*np.argmax(Q_value[index_State])\n",
    "            X = X.reshape(1,t+d+3*m)\n",
    "            target = target.reshape(1,1)\n",
    "            model.fit(X,target, epochs = 1, verbose = 0)\n",
    "            Q_value = prediction_DQN(m,t,d,actions,X,index_State, Q_value)\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "    if epi_len % 50 == 0:\n",
    "        print(\"Reward:{},Episode:{}, Epsilon:{}\".format(reward,epi_len,epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
