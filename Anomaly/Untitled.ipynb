{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5baed60061d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMODEL_OUT_DIR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatOneHotDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anomaly\\anomalydetector\\dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTMP_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENTLOG_DIR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0manomalydetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessmining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEventLog\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anomaly\\anomalydetector\\processmining.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import _pickle as pickle\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from anomalydetector import MODEL_OUT_DIR\n",
    "from anomalydetector.dataset import Dataset\n",
    "from anomalydetector.dataset import FlatOneHotDataset\n",
    "from anomalydetector.dataset import OneHotDataset\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    This is a boilerplate anomaly detector that only provides simple serialization and deserialization methods\n",
    "    using pickle. Other classes can inherit the behavior. They will have to implement both the fit and the predict\n",
    "    method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, abbreviation=None):\n",
    "        self.model = None\n",
    "\n",
    "        self.abbreviation = abbreviation\n",
    "        self.dataset = Dataset()\n",
    "\n",
    "        if model is not None:\n",
    "            self.load(model)\n",
    "\n",
    "    def load(self, model):\n",
    "        \"\"\"\n",
    "        Load a class instance from a pickle file. If no extension or absolute path are given the method assumes the\n",
    "        file to be located inside the MODEL_OUT_DIR. It will also add the .pkl extension.\n",
    "\n",
    "        :param model: path to the pickle file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # set extension\n",
    "        if not model.endswith('.pkl'):\n",
    "            model += '.pkl'\n",
    "\n",
    "        # set parameters\n",
    "        if not os.path.isabs(model):\n",
    "            model = os.path.join(MODEL_OUT_DIR, model)\n",
    "\n",
    "        # load model\n",
    "        self.model = pickle.load(open(model, 'rb'))\n",
    "\n",
    "    def save(self, file_name):\n",
    "        \"\"\"\n",
    "        Save the class instance using pickle.\n",
    "\n",
    "        The filename will have the following structure: <file_name>_<self.abbreviation>_<current_datetime>.pkl\n",
    "\n",
    "        :param file_name: custom file name\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            file_name = '{}_{}_{}.pkl'.format(file_name, self.abbreviation, date)\n",
    "            with open(os.path.join(MODEL_OUT_DIR, file_name), 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "        else:\n",
    "            raise Exception('No model has been trained yet.')\n",
    "\n",
    "    def fit(self, eventlog_name):\n",
    "        \"\"\"\n",
    "        This method must be implemented by the subclasses.\n",
    "\n",
    "        :param eventlog_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_proba(self, eventlog_name):\n",
    "        \"\"\"\n",
    "        This method must be implemented by the subclasses.\n",
    "\n",
    "        :param eventlog_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class RandomAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, model=None):\n",
    "        super().__init__(model=model, abbreviation='baseline')\n",
    "\n",
    "        self._labels = None\n",
    "        self._trace_probabilities = None\n",
    "        self._transition_probabilities = None\n",
    "\n",
    "    def fit(self, model):\n",
    "        model = os.path.basename(model)\n",
    "        if '_' in model:\n",
    "            dataset_name = model[:model.find('_')]\n",
    "        else:\n",
    "            dataset_name = model\n",
    "\n",
    "        dataset = FlatOneHotDataset()\n",
    "        x, y, labels = dataset.load(dataset_name)\n",
    "\n",
    "        # get labels\n",
    "        trace_labels = [any(l) for l in y]\n",
    "        transition_labels = np.concatenate(y)\n",
    "\n",
    "        # trace label probabilities\n",
    "        self._labels, counts = np.unique(trace_labels, return_counts=True)\n",
    "        self._trace_probabilities = counts / np.sum(counts)\n",
    "\n",
    "        # transition label probabilities\n",
    "        _, counts = np.unique(transition_labels, return_counts=True)\n",
    "        self._transition_probabilities = counts / np.sum(counts)\n",
    "\n",
    "    def predict_proba(self, traces):\n",
    "        predictions = np.random.choice(self._labels, size=(traces.shape[0], traces.shape[1] - 1),\n",
    "                                       p=self._trace_probabilities)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class SlidingWindowAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, model=None, k=2):\n",
    "        super().__init__(model=model, abbreviation='sw')\n",
    "\n",
    "        self.transitions = None\n",
    "        self.threshold = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, traces):\n",
    "        self.transitions = {}\n",
    "        num_transitions = 0\n",
    "        for trace in traces:\n",
    "            for transition in zip(*[trace[i:] for i in range(self.k)]):\n",
    "                t = '--'.join(transition)\n",
    "                num_transitions += 1\n",
    "                if t in self.transitions.keys():\n",
    "                    self.transitions[t]['probability'] += 1\n",
    "                else:\n",
    "                    self.transitions[t] = {\n",
    "                        'probability': 1,\n",
    "                        'transition': transition\n",
    "                    }\n",
    "\n",
    "        for key in self.transitions.keys():\n",
    "            self.transitions[key]['probability'] /= num_transitions\n",
    "\n",
    "        self.threshold = np.mean([transition['probability'] for transition in self.transitions.values()])\n",
    "\n",
    "    def predict_proba(self, traces):\n",
    "        trace_lens = [len(trace) for trace in traces]\n",
    "        num_windows = np.max(trace_lens) - (self.k - 1)\n",
    "\n",
    "        trace_pred = np.empty((len(traces), num_windows))\n",
    "        trace_pred[:] = np.infty\n",
    "        for i, trace in enumerate(traces):\n",
    "            for j, transition in enumerate(zip(*[trace[i:] for i in range(self.k)])):\n",
    "                t = '--'.join(transition)\n",
    "                if t in self.transitions:\n",
    "                    trace_pred[i, j] = self.transitions[t]['probability']\n",
    "                else:\n",
    "                    trace_pred[i, j] = 0\n",
    "        return trace_pred\n",
    "\n",
    "\n",
    "class OneClassSVMAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, model=None):\n",
    "        super().__init__(model=model, abbreviation='one-class-svm')\n",
    "\n",
    "    def fit(self, traces):\n",
    "        from sklearn.svm import OneClassSVM\n",
    "        self.model = OneClassSVM(nu=0.8, kernel='poly')\n",
    "        self.model.fit(traces)\n",
    "\n",
    "    def predict(self, traces):\n",
    "        # This only returns predictions for traces\n",
    "        return self.model.predict(traces)\n",
    "\n",
    "\n",
    "class HMMAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, n_components=4, model=None):\n",
    "        super().__init__(model=model, abbreviation='hmm')\n",
    "\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, traces, trace_lens):\n",
    "        from hmmlearn.hmm import GaussianHMM\n",
    "        self.model = GaussianHMM(n_components=self.n_components, covariance_type=\"diag\", n_iter=100)\n",
    "        self.model.fit(traces, trace_lens)\n",
    "\n",
    "    def predict(self, traces, trace_lens):\n",
    "        x = np.split(traces, np.cumsum(trace_lens)[:-1])\n",
    "\n",
    "        log_probs = []\n",
    "        for seq in x:\n",
    "            log_probs.append(self.model.decode(seq)[0])\n",
    "\n",
    "        return np.array(log_probs)\n",
    "\n",
    "\n",
    "class NNAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, model=None, abbreviation=None):\n",
    "        super().__init__(model=model, abbreviation=abbreviation)\n",
    "\n",
    "    def load(self, model):\n",
    "        # set extension\n",
    "        if not model.endswith('.h5'):\n",
    "            model += '.h5'\n",
    "\n",
    "        # set parameters\n",
    "        if not os.path.isabs(model):\n",
    "            model = os.path.join(MODEL_OUT_DIR, model)\n",
    "\n",
    "        # load model\n",
    "        from keras.models import load_model\n",
    "        try:\n",
    "            self.model = load_model(model)\n",
    "        except Exception:\n",
    "            print(model, 'failed to load')\n",
    "\n",
    "    def save(self, dataset_name=None):\n",
    "        if self.model:\n",
    "            date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            file_name = '{}_{}_{}.h5'.format(dataset_name, self.abbreviation, date)\n",
    "            self.model.save(os.path.join(MODEL_OUT_DIR, file_name))\n",
    "            print('model saved')\n",
    "        else:\n",
    "            raise Exception('No net has been trained yet.')\n",
    "\n",
    "\n",
    "class DAEAnomalyDetector(NNAnomalyDetector):\n",
    "    def __init__(self, model=None):\n",
    "        super().__init__(model=model, abbreviation='dae')\n",
    "\n",
    "        self.dataset = FlatOneHotDataset()\n",
    "\n",
    "    def fit(self, eventlog_name):\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.contrib.keras.python.keras.engine import Input, Model\n",
    "        from tensorflow.contrib.keras.python.keras.layers import Dense, GaussianNoise, Dropout\n",
    "\n",
    "        # load data\n",
    "        features = self.dataset.load(eventlog_name) #12500,372\n",
    "\n",
    "        # parameters\n",
    "        input_size = features.shape[1] #372\n",
    "        hidden_size = np.round(input_size * 4)\n",
    "\n",
    "        # input layer\n",
    "        input_layer = Input(shape=(input_size,), name='input') # this input is basically a tensor function, check it out\n",
    "\n",
    "        # hidden layer\n",
    "        hid = Dense(hidden_size, activation=tf.nn.relu)(GaussianNoise(0.1)(input_layer))\n",
    "        hid = Dense(hidden_size, activation=tf.nn.relu)(Dropout(0.5)(hid))\n",
    "        hid = Dense(hidden_size, activation=tf.nn.relu)(Dropout(0.5)(hid))\n",
    "        hid = Dense(hidden_size, activation=tf.nn.relu)(Dropout(0.5)(hid))\n",
    "        hid = Dense(hidden_size, activation=tf.nn.relu)(Dropout(0.5)(hid))\n",
    "\n",
    "        # output layer\n",
    "        output_layer = Dense(input_size, activation='linear')(Dropout(0.5)(hid))\n",
    "\n",
    "        # build model\n",
    "        self.model = Model(inputs=input_layer, outputs=output_layer)#this Model is also from tensorflow.contrib.keras\n",
    "\n",
    "        # compile model\n",
    "        self.model.compile(\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=0.0001),\n",
    "            loss=tf.losses.mean_squared_error\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        self.model.fit(\n",
    "            features,\n",
    "            features,\n",
    "            batch_size=100,\n",
    "            epochs= 2, # changing it from 100 to 20\n",
    "            validation_split=0.2,\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, eventlog_name):\n",
    "        \"\"\"\n",
    "        Calculate the anomaly score for each event attribute in each trace.\n",
    "        Anomaly score here is the mean squared error.\n",
    "\n",
    "        :param traces: traces to predict\n",
    "        :return:\n",
    "            anomaly_scores: anomaly scores for each attribute;\n",
    "                            shape is (#traces, max_trace_length - 1, #attributes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.dataset.load(eventlog_name)\n",
    "\n",
    "        # get event length\n",
    "        event_len = np.sum(self.dataset.attribute_dims - 1).astype(int)\n",
    "\n",
    "        # init anomaly scores array\n",
    "        anomaly_scores = np.zeros((features.shape[0], self.dataset.max_len - 1, len(self.dataset.attribute_dims))) #12500,11,2\n",
    "\n",
    "        # get predictions\n",
    "        predictions = self.model.predict(features)\n",
    "        errors = (predictions - features) ** 2\n",
    "\n",
    "        # remove the BOS event\n",
    "        errors = errors[:, event_len:]\n",
    "\n",
    "        # split the errors according to the attribute dims\n",
    "        split = np.cumsum(np.tile(self.dataset.attribute_dims - 1, self.dataset.max_len - 1), dtype=int)[:-1]\n",
    "        errors = np.split(errors, split, axis=1)\n",
    "        errors = np.array([np.mean(a, axis=1) for a in errors])\n",
    "\n",
    "        for i in range(len(self.dataset.attribute_dims)):\n",
    "            error = errors[i::len(self.dataset.attribute_dims)]\n",
    "            anomaly_scores[:, :, i] = error.T\n",
    "        anomalyasscores = anomaly_scores\n",
    "        print('the anomaly scores is:\\n', anomaly_scores[:20])\n",
    "\n",
    "        # TODO: Normalize the anomaly_scores to lie between 0 and 1\n",
    "        return -anomaly_scores\n",
    "\n",
    "\n",
    "class LSTMAnomalyDetector(NNAnomalyDetector):\n",
    "    def __init__(self, model=None, embedding=True):\n",
    "        self.embedding = embedding\n",
    "        self.distributions = None\n",
    "\n",
    "        super().__init__(model, abbreviation='lstm-emb' if embedding else 'lstm')\n",
    "\n",
    "        if not self.embedding:\n",
    "            self.dataset = OneHotDataset()\n",
    "        else:\n",
    "            self.dataset = Dataset()\n",
    "\n",
    "    def load(self, model):\n",
    "        # TODO: perhaps there is a better way to do this. We are probably gonna kill off the non-embedding version anyway.\n",
    "        super().load(model)\n",
    "        if 'lstm-emb' not in model:\n",
    "            self.embedding = False\n",
    "            self.dataset = OneHotDataset()\n",
    "\n",
    "    def fit(self, eventlog_name):\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.contrib.keras.python.keras.engine import Input, Model\n",
    "        from tensorflow.contrib.keras.python.keras.layers import Dense, Dropout, LSTM, Embedding, merge, Masking\n",
    "\n",
    "        # load data\n",
    "        features, targets = self.dataset.load(eventlog_name, train=True)\n",
    "\n",
    "        # input layers\n",
    "        inputs = []\n",
    "        layers = []\n",
    "\n",
    "        if self.embedding:\n",
    "            with tf.device('/cpu:0'):\n",
    "                # split attributes\n",
    "                features = [features[:, :, i] for i in range(features.shape[2])]\n",
    "\n",
    "                for i, t in enumerate(features):\n",
    "                    voc_size = np.array(self.dataset.attribute_dims[i]) + 1  # we start at 1, hence +1\n",
    "                    emb_size = np.floor(voc_size / 2.0).astype(int)\n",
    "                    i = Input(shape=(None, *t.shape[2:]))\n",
    "                    x = Embedding(input_dim=voc_size, output_dim=emb_size, input_length=t.shape[1], mask_zero=True)(i)\n",
    "                    inputs.append(i)\n",
    "                    layers.append(x)\n",
    "\n",
    "                # merge layers\n",
    "                x = merge.concatenate(layers)\n",
    "\n",
    "        else:\n",
    "            # input layer\n",
    "            i = Input(shape=(None, *features.shape[2:]))\n",
    "            x = Masking(mask_value=0)(i)\n",
    "            inputs.append(i)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(64, implementation=2)(x)\n",
    "\n",
    "        # shared hidden layer\n",
    "        x = Dense(512, activation=tf.nn.relu)(x)\n",
    "        x = Dense(512, activation=tf.nn.relu)(Dropout(0.5)(x))\n",
    "\n",
    "        # hidden layers per attribute\n",
    "        outputs = []\n",
    "        for i, l in enumerate(targets):\n",
    "            o = Dense(256, activation=tf.nn.relu)(Dropout(0.5)(x))\n",
    "            o = Dense(256, activation=tf.nn.relu)(Dropout(0.5)(o))\n",
    "            o = Dense(l.shape[1], activation=tf.nn.softmax)(Dropout(0.5)(o))\n",
    "            outputs.append(o)\n",
    "\n",
    "        # build model\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # compile model\n",
    "        self.model.compile(\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=0.0001),\n",
    "            loss='categorical_crossentropy'\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        self.model.fit(\n",
    "            features,\n",
    "            targets,\n",
    "            batch_size=100,\n",
    "            epochs=20, # reduce the number of epochs from 100 to 20 for faster debugging process\n",
    "            validation_split=0.2,\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, eventlog_name):\n",
    "        \"\"\"\n",
    "        Calculate the anomaly score and the probability distribution for each event in each trace.\n",
    "        Anomaly score here is the probability of that event occurring given all events before.\n",
    "\n",
    "        :param traces: traces to predict\n",
    "        :return:\n",
    "            anomaly_scores: anomaly scores for each attribute;\n",
    "                            shape is (#traces, max_trace_length - 1, #attributes)\n",
    "\n",
    "            distributions: probability distributions for each event and attribute;\n",
    "                           list of np.arrays with shape (#traces, max_trace_length - 1, #attribute_classes),\n",
    "                           one np.array for each attribute, hence list len is #attributes\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_all_subsequences(sequence):\n",
    "            \"\"\"\n",
    "            Calculate all subsequences for a given sequence after removing the padding (0s).\n",
    "\n",
    "            :param sequence:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "\n",
    "            num_subsequences = np.sum(np.any(sequence != 0, axis=1)) - 1  # remove padding and calculate num subseqs\n",
    "            subsequences = np.zeros((num_subsequences, sequence.shape[0], sequence.shape[1]))  # init array\n",
    "            next_events = sequence[1:num_subsequences + 1]  # get next event\n",
    "\n",
    "            for i in np.arange(num_subsequences):\n",
    "                length = num_subsequences - i\n",
    "                subsequences[i, :length, :] = sequence[:length, :]\n",
    "\n",
    "            return subsequences[::-1], next_events\n",
    "\n",
    "        # load data\n",
    "        features, _ = self.dataset.load(eventlog_name, train=False)\n",
    "\n",
    "        # anomaly scores for attributes\n",
    "        # shape is (#traces, max_len_trace - 1, #attributes)\n",
    "        # we do not predict the BOS activity, hence the -1\n",
    "        anomaly_scores = np.ones((features.shape[0], features.shape[1] - 1, len(self.dataset.attribute_dims)))\n",
    "\n",
    "        # distributions for each attribute\n",
    "        attr_dims = np.array([int(o.shape[1]) for o in self.model.output])\n",
    "        self.distributions = [np.ones((features.shape[0], features.shape[1] - 1, attr_dim)) for attr_dim in attr_dims]\n",
    "\n",
    "        sub_sequences = []\n",
    "        next_events = []\n",
    "        for i, trace in enumerate(features):\n",
    "            s, n = _get_all_subsequences(trace)\n",
    "            sub_sequences.append(s)\n",
    "            next_events.append(n)\n",
    "\n",
    "        sub_sequences = np.vstack(sub_sequences)\n",
    "        next_events = np.vstack(next_events).astype(int)\n",
    "\n",
    "        if self.embedding:\n",
    "            sub_sequences = [sub_sequences[:, :, i] for i in range(sub_sequences.shape[2])]\n",
    "            next_events = [next_events[:, i] - 1 for i in range(next_events.shape[1])]\n",
    "        else:\n",
    "            offset = np.concatenate([[0], np.cumsum(attr_dims)[:-1]])\n",
    "            n = np.zeros((next_events.shape[0], attr_dims.shape[0]), dtype=int)\n",
    "            for index, next_event in enumerate(next_events):\n",
    "                n[index] = np.where(next_event == 1)[0] - offset\n",
    "            next_events = [n[:, i] for i in range(n.shape[1])]\n",
    "\n",
    "        cumsum = np.cumsum(self.dataset.trace_lens - 1)\n",
    "        cumsum2 = np.concatenate(([0], cumsum[:-1]))\n",
    "        offsets = np.dstack((cumsum2, cumsum))[0]\n",
    "        dist = self.model.predict(sub_sequences)\n",
    "\n",
    "        for i, _n in enumerate(next_events):\n",
    "            scores = dist[i][range(dist[i].shape[0]), _n]\n",
    "            for j, trace_len in enumerate(self.dataset.trace_lens - 1):\n",
    "                start, end = offsets[j]\n",
    "                anomaly_scores[j][:trace_len, i] = scores[start:end]\n",
    "                self.distributions[i][j, :trace_len] = dist[i][start:end]\n",
    "\n",
    "        temp_anomaly_scores = anomaly_scores\n",
    "\n",
    "        return anomaly_scores\n",
    "\n",
    "\n",
    "class RNNGRUAnomalyDetector(NNAnomalyDetector):\n",
    "    def __init__(self, model=None, embedding=True):\n",
    "        self.dataset = Dataset()\n",
    "        super().__init__(model, abbreviation='RNNGRU')\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def load(self, model):\n",
    "        super().load(model)\n",
    "\n",
    "    def fit(self, eventlog_name):\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.contrib.keras.python.keras.engine import Input, Model\n",
    "        from tensorflow.contrib.keras.python.keras.layers import Dense, Dropout, GRU, Embedding, merge, Masking\n",
    "\n",
    "        features, targets = self.dataset.load(eventlog_name, train=True)\n",
    "        inputs = []\n",
    "        layers = []\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            # split attributes\n",
    "            features = [features[:, :, i] for i in range(features.shape[2])]\n",
    "\n",
    "            for i, t in enumerate(features):\n",
    "                voc_size = np.array(self.dataset.attribute_dims[i]) + 1  # we start at 1, hence +1\n",
    "                emb_size = np.floor(voc_size / 2.0).astype(int)\n",
    "\n",
    "                i = Input(shape=(None, *t.shape[2:]))\n",
    "                x = Embedding(input_dim=voc_size, output_dim=emb_size, input_length=t.shape[1], mask_zero=True)(i)\n",
    "                inputs.append(i)\n",
    "                layers.append(x)\n",
    "\n",
    "            # merge layers\n",
    "            x = merge.concatenate(layers)\n",
    "\n",
    "        x = GRU(64, implementation=2)(x)\n",
    "\n",
    "        # shared hidden layer\n",
    "        x = Dense(512, activation=tf.nn.relu)(x)\n",
    "        x = Dense(512, activation=tf.nn.relu)(Dropout(0.5)(x))\n",
    "\n",
    "        # hidden layers per attribute\n",
    "        outputs = []\n",
    "        for i, l in enumerate(targets):\n",
    "            o = Dense(256, activation=tf.nn.relu)(Dropout(0.5)(x))\n",
    "            o = Dense(256, activation=tf.nn.relu)(Dropout(0.5)(o))\n",
    "            o = Dense(l.shape[1], activation=tf.nn.softmax)(Dropout(0.5)(o))\n",
    "            outputs.append(o)\n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # compile model\n",
    "\n",
    "        # old setting : optimizers from tensorflow\n",
    "\n",
    "        # self.model.compile(\n",
    "        # optimizer=tf.train.AdamOptimizer(learning_rate=0.0001),\n",
    "        # loss='categorical_crossentropy'\n",
    "        # )\n",
    "\n",
    "        # new setting : optimizers from keras\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer='Adadelta',\n",
    "            loss='categorical_crossentropy'\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        self.model.fit(\n",
    "            features,\n",
    "            targets,\n",
    "            batch_size=100,\n",
    "            epochs=100,\n",
    "            validation_split=0.2,\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, eventlog_name):\n",
    "        \"\"\"\n",
    "        Calculate the anomaly score and the probability distribution for each event in each trace.\n",
    "        Anomaly score here is the probability of that event occurring given all events before.\n",
    "\n",
    "        :param traces: traces to predict\n",
    "        :return:\n",
    "            anomaly_scores: anomaly scores for each attribute;\n",
    "                    shape is (#traces, max_trace_length - 1, #attributes)\n",
    "\n",
    "            distributions: probability distributions for each event and attribute;\n",
    "                   list of np.arrays with shape (#traces, max_trace_length - 1, #attribute_classes),\n",
    "                   one np.array for each attribute, hence list len is #attributes\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_all_subsequences(sequence):\n",
    "            \"\"\"\n",
    "            Calculate all subsequences for a given sequence after removing the padding (0s).\n",
    "\n",
    "            :param sequence:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "\n",
    "            num_subsequences = np.sum(np.any(sequence != 0, axis=1)) - 1  # remove padding and calculate num subseqs\n",
    "            subsequences = np.zeros((num_subsequences, sequence.shape[0], sequence.shape[1]))  # init array\n",
    "            next_events = sequence[1:num_subsequences + 1]  # get next event\n",
    "\n",
    "            for i in np.arange(num_subsequences):\n",
    "                length = num_subsequences - i\n",
    "                subsequences[i, :length, :] = sequence[:length, :]\n",
    "\n",
    "            return subsequences[::-1], next_events\n",
    "\n",
    "        # load data\n",
    "        features, _ = self.dataset.load(eventlog_name, train=False)\n",
    "\n",
    "        # anomaly scores for attributes\n",
    "        # shape is (#traces, max_len_trace - 1, #attributes)\n",
    "        # we do not predict the BOS activity, hence the -1\n",
    "        anomaly_scores = np.ones((features.shape[0], features.shape[1] - 1, len(self.dataset.attribute_dims)))\n",
    "\n",
    "        # distributions for each attribute\n",
    "        attr_dims = np.array([int(o.shape[1]) for o in self.model.output])\n",
    "        self.distributions = [np.ones((features.shape[0], features.shape[1] - 1, attr_dim)) for attr_dim in attr_dims]\n",
    "\n",
    "        sub_sequences = []\n",
    "        next_events = []\n",
    "\n",
    "        for i, trace in enumerate(features):\n",
    "            s, n = _get_all_subsequences(trace)\n",
    "            sub_sequences.append(s)\n",
    "            next_events.append(n)\n",
    "\n",
    "        sub_sequences = np.vstack(sub_sequences)\n",
    "        next_events = np.vstack(next_events).astype(int)\n",
    "\n",
    "        if self.embedding:\n",
    "            sub_sequences = [sub_sequences[:, :, i] for i in range(sub_sequences.shape[2])]\n",
    "            next_events = [next_events[:, i] - 1 for i in range(next_events.shape[1])]\n",
    "        else:\n",
    "            offset = np.concatenate([[0], np.cumsum(attr_dims)[:-1]])\n",
    "            n = np.zeros((next_events.shape[0], attr_dims.shape[0]), dtype=int)\n",
    "            for index, next_event in enumerate(next_events):\n",
    "                n[index] = np.where(next_event == 1)[0] - offset\n",
    "                next_events = [n[:, i] for i in range(n.shape[1])]\n",
    "\n",
    "        cumsum = np.cumsum(self.dataset.trace_lens - 1)\n",
    "        cumsum2 = np.concatenate(([0], cumsum[:-1]))\n",
    "        offsets = np.dstack((cumsum2, cumsum))[0]\n",
    "        dist = self.model.predict(sub_sequences)\n",
    "\n",
    "        for i, _n in enumerate(next_events):\n",
    "            scores = dist[i][range(dist[i].shape[0]), _n]\n",
    "            for j, trace_len in enumerate(self.dataset.trace_lens - 1):\n",
    "                start, end = offsets[j]\n",
    "                anomaly_scores[j][:trace_len, i] = scores[start:end]\n",
    "                self.distributions[i][j, :trace_len] = dist[i][start:end]\n",
    "\n",
    "        return anomaly_scores\n",
    "        \n",
    "        \n",
    "class Clustering(Dataset):\n",
    "    def __init__(self, dataset_name=None):\n",
    "        self.dataset = Dataset()\n",
    "        self.encoding = OneHotDataset() # make changes here\n",
    "        #self.encoding = FlatOneHotDataset()\n",
    "\n",
    "    def Kkmeans_clustering(self,features = None):\n",
    "        if features is None:\n",
    "            features, targets = self.dataset.load( dataset_name='p2p-0.1-1')\n",
    "        #if features is None:\n",
    "            #features,targets = self.encoding.load(dataset_name= 'p2p-0.1-1')\n",
    "        #Feature_matrix = np.zeros((features.shape[0], features.shape[1]*features.shape[2]))\n",
    "        Feature_matrix = np.zeros((features.shape[0], features.shape[1]*features.shape[2] ))\n",
    "        for a, i in enumerate(features):\n",
    "            Feature_matrix[a] = np.resize(i, (1, features.shape[1]*features.shape[2])) # multiply it with features.shape[2] for other features\n",
    "        kmeans = KMeans(n_clusters=6, random_state=0).fit(scale(Feature_matrix))\n",
    "        \n",
    "        plt.scatter(Feature_matrix[:, 0], Feature_matrix[:, 1], c=kmeans.labels_.astype(np.float))\n",
    "        centers = kmeans.cluster_centers_\n",
    "        labels = kmeans.labels_\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print ('count is:\\n',np.asarray((unique, counts)).T)\n",
    "\n",
    "        '''\n",
    "        # Computing the purity of cluster with traces with Incorrect user anomaly##\n",
    "\n",
    "        Incorrect_user_true = L[(np.where(kmeans.labels_ == 4))]\n",
    "        Incorrect_user_pred = np.full(len((np.where(kmeans.labels_ == 4))[0]), 1)\n",
    "        Purity_score = sum(Incorrect_user_pred == Incorrect_user_true) / len(Incorrect_user_true) * 100\n",
    "        print(\"The purity of cluster with incorrect user anomaly is %f %% \" % Purity_score)\n",
    "        '''\n",
    "        return(kmeans.labels_, plt.show())\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    #clustering = Clustering()  # instantiating our object without attributes\n",
    "    anomalyinstance = DAEAnomalyDetector() # make changes here\n",
    "    y = anomalyinstance.fit(eventlog_name='p2p-0.1-1') # make changes here\n",
    "    z = anomalyinstance.predict_proba(eventlog_name='p2p-0.1-1')# make changes here\n",
    "    #lstm = LSTMAnomalyDetector()\n",
    "    #fit = lstm.fit(eventlog_name='p2p-0.1-1')\n",
    "    #pred = lstm.predict_proba(eventlog_name='p2p-0.1-1')\n",
    "    #print(pred)\n",
    "    #x = clustering.Kkmeans_clustering() # insert eventlog_name here\n",
    "    print(z)\n",
    "    #print ('anomaly scores be like:\\n', z)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
