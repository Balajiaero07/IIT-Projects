{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import fisher_exact\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import pickle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_file.csv\")\n",
    "data1 = pd.read_csv(\"test_file.csv\")\n",
    "Topic_features  = pd.read_csv(\"Topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_Encoding(data):\n",
    "    Permit = pd.get_dummies(data['Permit Type'],prefix='Permit Type',drop_first=True)\n",
    "    Action = pd.get_dummies(data['Action Type'],prefix='Action Type',drop_first=True)\n",
    "    Work = pd.get_dummies(data['Work Type'],prefix='Work Type',drop_first=True)\n",
    "    Statu = pd.get_dummies(data['Status'],prefix='Status',drop_first=True)\n",
    "    dummy_encode =  pd.concat([Permit,Action,Work,Statu],axis=1)\n",
    "    return dummy_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_features(data):\n",
    "    completed_Submission = ~data['Application Date'].isna()\n",
    "    Issue_data = ~data['Issue Date'].isna()\n",
    "    Under_review = (completed_Submission == True) & (Issue_data == False)\n",
    "    Final_Date = ~data['Final Date'].isna()\n",
    "    Under_inspection  = (Issue_data == True) & (Final_Date == False)\n",
    "    not_issued = data['Expiration Date'].isna()\n",
    "    total_date = (pd.to_datetime(data['Expiration Date']) - pd.to_datetime(data['Application Date'])).astype('timedelta64[D]') \n",
    "    total_date[total_date.isna()] =  max(total_date)*2\n",
    "    date_variables = pd.concat([completed_Submission,Issue_data,Under_review,Final_Date,Under_inspection,not_issued,total_date],axis=1)\n",
    "    return date_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_labels(data,c1,c2):\n",
    "    X1 = data['Longitude'].fillna(0)\n",
    "    X2 = data['Latitude'].fillna(0)\n",
    "    X = pd.concat([X1,X2],axis=1)\n",
    "    kmeanModel = KMeans(n_clusters=c1)\n",
    "    kmeanModel.fit(X)\n",
    "    gmm = GaussianMixture(n_components=c2)\n",
    "    gmm.fit(X)\n",
    "    return kmeanModel,gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractor_freq(data):\n",
    "    Freq = data['Contractor'].fillna(0)\n",
    "    Freq[Freq == 'SEATTLE HOUSING AUTH GENERAL'] = 1\n",
    "    Freq[Freq == 'SEATTLE SCHOOL DISTRICT (A&S)'] = 2\n",
    "    Freq[Freq == 'U OF W BUILDING PERMIT'] = 3\n",
    "    Freq[Freq == 'SEATTLE PARKS DEPT'] = 4\n",
    "    Freq[(Freq == 'BURGESS DESIGN, INC') | (Freq == 'CITY OF SEA F&FD') | \n",
    "        (Freq == 'SAGE HOMES NORTHWEST, LLC')| (Freq == 'IA/INTERIOR ARCHITECTS')  |(Freq == 'SOUND SEISMIC')| (Freq == 'AMAZON.COM')] = 5\n",
    "    Freq[(Freq == 'PORT OF SEATTLE ENGINEERING') | (Freq == 'BLANKET: BANK OF AMERICA TOWER') | \n",
    "        (Freq == 'CITY INVESTORS')| (Freq == 'GREEN CANOPY HOMES')  |(Freq == 'POLYGON WLH LLC')] = 6\n",
    "    Freq[~((Freq == 1) | (Freq == 2) | (Freq == 3) | (Freq == 0) | (Freq == 4) | (Freq == 5) | (Freq == 6))]= 7 \n",
    "    return Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaled=data['Master Use Permit'].fillna(0)\n",
    "    scaler.fit(scaled.reshape(-1,1))\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data(data):\n",
    "    date_variables = date_features(data)\n",
    "    Freq = pd.DataFrame({'Frequ': contractor_freq(data)})\n",
    "    Processed_data = pd.concat([date_variables,Freq],axis=1)\n",
    "    return Processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_topic = Topic_features[0:len(data2)]\n",
    "Test_topic = Topic_features[len(data2):len(data2)+len(data1)]\n",
    "Train_topic.drop(Train_topic.columns[0],axis=1,inplace=True)\n",
    "Test_topic.drop(Test_topic.columns[0],axis=1,inplace=True)\n",
    "Test_topic.reset_index(inplace=True)\n",
    "kmeanModel, gmm = cluster_labels(data,3,5)\n",
    "X1 = data['Longitude'].fillna(0)\n",
    "X2 = data['Latitude'].fillna(0)\n",
    "X = pd.concat([X1,X2],axis=1)\n",
    "Cluster_L1 = kmeanModel.predict(X)\n",
    "Cluster_L2 = gmm.predict(X)\n",
    "clusters_features = pd.concat([pd.DataFrame({'KNN': Cluster_L1}),pd.DataFrame({'EM':Cluster_L2}),X],axis=1)\n",
    "scaler = scaling(data)\n",
    "data2 = data.drop('Category', axis=1) \n",
    "data_temp = pd.concat([data2,data1],axis=0)\n",
    "dummy_encode_train = dummies_Encoding(data_temp)\n",
    "dummy_encode = dummy_encode_train.iloc[0:len(data2)]\n",
    "dummy_encode_test = dummy_encode_train.iloc[len(data2):len(data2)+len(data1)]\n",
    "scaled_Mast = scaler.transform(data['Master Use Permit'].fillna(0).reshape(-1,1))\n",
    "scaled_Mast = pd.DataFrame(scaled_Mast)\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=9)\n",
    "rus = RandomUnderSampler(random_state=8)\n",
    "smote = SMOTE(random_state=9, kind=\"borderline2\")\n",
    "Processed_data1 = pd.concat([dummy_encode,processed_data(data),clusters_features,scaled_Mast,Train_topic],axis=1)\n",
    "#undersampling and over sampling\n",
    "\n",
    "X_sample_0, y_sample_0 = ros.fit_sample(Processed_data1, data['Category'])\n",
    "X_sample_s, y_sample_s = smote.fit_sample(Processed_data1, data['Category'])\n",
    "X_sample_u, y_sample_u =  rus.fit_sample(Processed_data1, data['Category'])\n",
    "X_train_O,X_test_O,y_train_O,y_test_O = train_test_split(X_sample_0,y_sample_0,test_size = 0.3)\n",
    "X_train_s,X_test_s,y_train_s,y_test_s = train_test_split(X_sample_s,y_sample_s,test_size = 0.3)\n",
    "X_train_u,X_test_u,y_train_u,y_test_u = train_test_split(X_sample_u,y_sample_u,test_size = 0.3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier \n",
    "def gridfunc(classifier, parameter, X_train, y_train):\n",
    "    clf = classifier\n",
    "    np.random.seed(9)\n",
    "    parameters = parameter\n",
    "    f1_scorer = make_scorer(f1_score,average='weighted')\n",
    "    # Run the grid search\n",
    "    grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "    grid_obj = grid_obj.fit(X_train, y_train) \n",
    "    return grid_obj          \n",
    "def hp_cv_scores(grid_obj):\n",
    "    grid_obj.cv_results_\n",
    "    mean_test_scores = grid_obj.cv_results_['mean_test_score']\n",
    "    mean_train_scores = grid_obj.cv_results_['mean_train_score']\n",
    "    plt.figure(figsize=(10,6)) \n",
    "    param_values =[str(x) for x in list(grid_obj.param_grid.items())[0][1]]\n",
    "    x = np.arange(1, len(param_values)+1)\n",
    "    plt.plot(x,mean_train_scores, c='r', label='Train set')\n",
    "    plt.xticks(x,param_values)\n",
    "    plt.plot(x,mean_test_scores,c='g', label='Test set')\n",
    "    plt.xlabel(list(grid_obj.param_grid.items())[0][1])\n",
    "    plt.ylabel('mean scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "classifier = RandomForestClassifier(random_state=9)\n",
    "grid = gridfunc(classifier, {'n_estimators': [10, 40, 60]} , X_sample_0, y_sample_0)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data preparation\n",
    "scaled_Mast1 = scaler.transform(data1['Master Use Permit'].fillna(0).reshape(-1,1))\n",
    "scaled_Mast1 = pd.DataFrame(scaled_Mast1)\n",
    "X1_t = data1['Longitude'].fillna(0)\n",
    "X2_t = data1['Latitude'].fillna(0)\n",
    "X_t = pd.concat([X1_t,X2_t],axis=1)\n",
    "Cluster_L1_t = kmeanModel.predict(X_t)\n",
    "Cluster_L2_t = gmm.predict(X_t)\n",
    "clusters_features_t = pd.concat([pd.DataFrame({'KNN': Cluster_L1_t}),pd.DataFrame({'EM':Cluster_L2_t}),X_t],axis=1)\n",
    "test_processed = pd.concat([dummy_encode_test,processed_data(data1),clusters_features_t,scaled_Mast1,Test_topic],axis=1)\n",
    "test_processed.drop(['index'], axis=1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_data1.to_csv('Training.csv')\n",
    "test_processed.to_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction for test data\n",
    "random_forest =  grid.predict(test_processed)\n",
    "RF_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':random_forest})],axis=1)\n",
    "RF_prediction.to_csv('prediction4.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting technique\n",
    "model = XGBClassifier(n_estimators=1000)\n",
    "model.fit(X_train_O, y_train_O)\n",
    "y_pred = model.predict(X_test_O)\n",
    "f1_score(y_test_O, y_pred, average='weighted')\n",
    "Boosting =  model.predict(test_processed.as_matrix())\n",
    "Boosting_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':Boosting})],axis=1)\n",
    "Boosting_prediction.to_csv('prediction2.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extractions from Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "ps = PorterStemmer()\n",
    "import string \n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_file.csv\")\n",
    "data1 = pd.read_csv(\"test_file.csv\")\n",
    "data_temp1 = pd.concat([data,data1],axis=0)\n",
    "data_temp1['Description'].fillna('No address',inplace= True)\n",
    "data_temp1.shape\n",
    "data_temp1['POS'] = pos_tag_sents(data_temp1['Description'].apply(word_tokenize).tolist())\n",
    "data_temp1['noun_words'] = np.nan\n",
    "data_temp1['noun_words_sentec'] = np.nan\n",
    "data_temp1 = data_temp1.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data_temp1)):\n",
    "    tagged = data_temp1.iloc[j,20]\n",
    "    noun_words = []\n",
    "    for item in tagged:\n",
    "        if item[1] == 'N' or item[1] == 'NN' or item[1] == 'NNP' or item[1] == 'NNS' or item[1] == 'NNPS':\n",
    "            noun_words.append(item[0])\n",
    "    data_temp1.iloc[j,21] = noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data_temp1)):\n",
    "    temp = ' '.join(word for word in data_temp1.iloc[j,21])\n",
    "    data_temp1.iloc[j,22] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000 #top 1000 features are selected\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=0.02,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_temp1['noun_words_sentec'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=9)\n",
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "ntopics = [5,10,20,40]\n",
    "fscore = []\n",
    "for i in range(len(ntopics)):\n",
    "    lda_model = LatentDirichletAllocation(n_topics=ntopics[i],               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "    lda_output = lda_model.fit_transform(tf)\n",
    "    Topic_distribution = DataFrame(data=lda_output)\n",
    "    X_sample_0, y_sample_0 = ros.fit_sample(Topic_distribution.iloc[0:len(data)], data['Category'])\n",
    "    X_train_O,X_test_O,y_train_O,y_test_O = train_test_split(X_sample_0,y_sample_0,test_size = 0.3)\n",
    "    classifier.fit(X_train_O,y_train_O)\n",
    "    y_pred = classifier.predict(X_test_O)\n",
    "    fscore.append(f1_score(y_test_O, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_topics=10,               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "lda_output = lda_model.fit_transform(tf)\n",
    "Topic_distribution = DataFrame(data=lda_output)\n",
    "Topic_distribution.to_csv(\"Topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_0, y_sample_0 = ros.fit_sample(Topic_distribution.iloc[0:len(data)], data['Category'])\n",
    "classifier.fit(X_sample_0,y_sample_0)\n",
    "y_pred = classifier.predict(Topic_distribution.iloc[len(data):len(data)+len(data1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':y_pred})],axis=1)\n",
    "Topic_prediction.to_csv('prediction3.csv', sep=',',index = False)\n",
    "Topic_distribution.rename(columns={0:'Topic0',1:'Topic1',2:'Topic2',\n",
    "                                  3:'Topic3',4:'Topic4',5:'Topic5',\n",
    "                                  6:'Topic6',7:'Topic7',8:'Topic8',\n",
    "                                  9:'Topic9'}, inplace = True)\n",
    "Topic_distribution.to_csv(\"Topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm   \n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_file.csv\")\n",
    "data1 = pd.read_csv(\"test_file.csv\")\n",
    "Train_features = pd.read_csv(\"Training.csv\")\n",
    "Test_features = pd.read_csv(\"Test.csv\")\n",
    "ros = RandomOverSampler(random_state=9)\n",
    "X_sample_0, y_sample_0 = ros.fit_sample(Train_features, data['Category'])\n",
    "X_train_O,X_test_O,y_train_O,y_test_O = train_test_split(X_sample_0,y_sample_0,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=8)\n",
    "X_sample_u, y_sample_u =  rus.fit_sample(Train_features, data['Category'])\n",
    "X_train_u,X_test_u,y_train_u,y_test_u = train_test_split(X_sample_u,y_sample_u,test_size = 0.3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.01, C=0.01).fit(X_train_O, y_train_O)\n",
    "y_pred = rbf.predict(X_test_O)\n",
    "f1_score(y_test_O, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm =  rbf.predict(Test_features.as_matrix())\n",
    "svm_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':svm})],axis=1)\n",
    "svm_prediction.to_csv('prediction12.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm =  rbf.predict(Test_features.as_matrix())\n",
    "svm_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':svm})],axis=1)\n",
    "svm_prediction.to_csv('prediction12.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=15000,n_jobs=-1)\n",
    "model.fit(X_train_O, y_train_O)\n",
    "y_pred = model.predict(X_test_O)\n",
    "f1_score(y_test_O, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting =  model.predict(Test_features.as_matrix())\n",
    "Boosting_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':Boosting})],axis=1)\n",
    "Boosting_prediction.to_csv('prediction12.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model\n",
    "def keras_model():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(250, input_dim = np.int(X_train_O.shape[1]),activation ='relu'))\n",
    "    model1.add(Dense(250,activation ='relu'))\n",
    "    model1.add(Dropout(0.5))\n",
    "    model1.add(Dense(250,activation ='relu'))\n",
    "    model1.add(Dropout(0.5))\n",
    "    model1.add(Dense(250,activation ='relu'))\n",
    "    model1.add(Dense(5,activation='softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model1\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_sample_0)\n",
    "encoded_Y = encoder.transform(y_sample_0)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "estimator = KerasClassifier(build_fn=keras_model, epochs=1, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=50)\n",
    "results = cross_val_score(estimator, X_sample_0, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = pd.read_csv(\"prediction1.csv\")\n",
    "pred2 = pd.read_csv(\"prediction2.csv\")\n",
    "pred3 = pd.read_csv(\"prediction3.csv\")\n",
    "pred4 = pd.read_csv(\"prediction4.csv\")\n",
    "pred5 = pd.read_csv(\"prediction5.csv\")\n",
    "pred6 = pd.read_csv(\"prediction10.csv\")\n",
    "pred7 = pd.read_csv(\"prediction7.csv\")\n",
    "final_pred = pd.concat([pred2['Category'],\n",
    "                       pred2['Category'],\n",
    "                       pred3['Category'],\n",
    "                       pred4['Category'],\n",
    "                       pred5['Category'],\n",
    "                       pred6['Category'],\n",
    "                       pred7['Category']],axis=1)\n",
    "labels = []\n",
    "for i in range(len(final_pred)):\n",
    "    labels.append(Counter(final_pred.loc[i]).most_common(1)[0][0])\n",
    "Max_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':labels})],axis=1)\n",
    "Max_prediction.to_csv('prediction11.csv', sep=',',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
