{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "ps = PorterStemmer()\n",
    "import string \n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55899, 19)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train_file.csv\")\n",
    "data1 = pd.read_csv(\"test_file.csv\")\n",
    "data_temp1 = pd.concat([data,data1],axis=0)\n",
    "data_temp1['Description'].fillna('No address',inplace= True)\n",
    "data_temp1.shape\n",
    "data_temp1['POS'] = pos_tag_sents(data_temp1['Description'].apply(word_tokenize).tolist())\n",
    "data_temp1['noun_words'] = np.nan\n",
    "data_temp1['noun_words_sentec'] = np.nan\n",
    "data_temp1 = data_temp1.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(data_temp1)):\n",
    "    tagged = data_temp1.iloc[j,20]\n",
    "    noun_words = []\n",
    "    for item in tagged:\n",
    "        if item[1] == 'N' or item[1] == 'NN' or item[1] == 'NNP' or item[1] == 'NNS' or item[1] == 'NNPS':\n",
    "            noun_words.append(item[0])\n",
    "    data_temp1.iloc[j,21] = noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(data_temp1)):\n",
    "    temp = ' '.join(word for word in data_temp1.iloc[j,21])\n",
    "    data_temp1.iloc[j,22] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_features = 1000 #top 1000 features are selected\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=0.02,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_temp1['noun_words_sentec'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=9)\n",
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "ntopics = [5,10,20,40]\n",
    "fscore = []\n",
    "for i in range(len(ntopics)):\n",
    "    lda_model = LatentDirichletAllocation(n_topics=ntopics[i],               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "    lda_output = lda_model.fit_transform(tf)\n",
    "    Topic_distribution = DataFrame(data=lda_output)\n",
    "    X_sample_0, y_sample_0 = ros.fit_sample(Topic_distribution.iloc[0:len(data)], data['Category'])\n",
    "    X_train_O,X_test_O,y_train_O,y_test_O = train_test_split(X_sample_0,y_sample_0,test_size = 0.3)\n",
    "    classifier.fit(X_train_O,y_train_O)\n",
    "    y_pred = classifier.predict(X_test_O)\n",
    "    fscore.append(f1_score(y_test_O, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\venkatachalam s\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_topics=10,               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "lda_output = lda_model.fit_transform(tf)\n",
    "Topic_distribution = DataFrame(data=lda_output)\n",
    "Topic_distribution.to_csv(\"Topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample_0, y_sample_0 = ros.fit_sample(Topic_distribution.iloc[0:len(data)], data['Category'])\n",
    "classifier.fit(X_sample_0,y_sample_0)\n",
    "y_pred = classifier.predict(Topic_distribution.iloc[len(data):len(data)+len(data1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topic_prediction = pd.concat([data1['Application/Permit Number'],pd.DataFrame({'Category':y_pred})],axis=1)\n",
    "Topic_prediction.to_csv('prediction3.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topic_distribution.rename(columns={0:'Topic0',1:'Topic1',2:'Topic2',\n",
    "                                  3:'Topic3',4:'Topic4',5:'Topic5',\n",
    "                                  6:'Topic6',7:'Topic7',8:'Topic8',\n",
    "                                  9:'Topic9'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topic_distribution.to_csv(\"Topic_distribution.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
