{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Goal A - [0,11],Goal B -[2,9],Goal C -[6,7]\n",
    "goal_c=7\n",
    "goal_r=6\n",
    "# core algorithm\n",
    "epsilon_value = 0.1   # assume\n",
    "alpha = 0.5  #learning rate\n",
    "gamma = 0.9  #discount rate\n",
    "wind_prob = 0.5  \n",
    "episodes = 5000 \n",
    "runs = 50  \n",
    "M = 10000\n",
    "lmbda=0.9\n",
    "steps=np.zeros((runs,episodes))\n",
    "G_all=np.zeros((runs,episodes))\n",
    "dum_a = np.array([0,1,2,3])\n",
    "grid_rewards = np.array([[0, 0, 0, 0,  0,  0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0,  0,  0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -1,  -1, -1, -1, -1, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -2,  -2, -2, -2, -1, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -2,  -3, -3, -2, -1, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -2,  -3, -2, -2, -1, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -2,  -3, -2, -1, -1, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -2,  -2, -2, -1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, -1,  -1,  -1, -1, -1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0,  0,  0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0,  0,  0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0,  0,  0, 0, 0, 0, 0, 0, 0]]).astype(\"float32\")\n",
    "grid_rewards[goal_r,goal_c]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statechangefn(act_sel,s_r,s_c):\n",
    "    dum_b=set(dum_a) - set([act_sel])\n",
    "    pr_action = np.zeros_like(dum_a).astype(float)\n",
    "    pr_action[act_sel,]=gamma\n",
    "    pr_action[list(dum_b)]=(1-gamma)/3\n",
    "    cum_prob = np.cumsum(pr_action)\n",
    "    sel_direction= np.digitize(random.uniform(0,1),cum_prob)\n",
    "    if sel_direction == 0:\n",
    "        s_new_r = s_r -1 \n",
    "        s_new_c = s_c\n",
    "    elif sel_direction == 1:\n",
    "        s_new_r = s_r  \n",
    "        s_new_c = s_c + 1         \n",
    "    elif sel_direction == 2:\n",
    "        s_new_r = s_r + 1\n",
    "        s_new_c = s_c        \n",
    "    elif sel_direction == 3:\n",
    "        s_new_r = s_r  \n",
    "        s_new_c = s_c - 1\n",
    "        \n",
    "    # States after hitting wall remains unchanged (ROWS)\n",
    "    if s_new_r == -1:\n",
    "        s_new_r = 0\n",
    "    elif s_new_r == 12:\n",
    "        s_new_r = 11\n",
    "        \n",
    "    #States after hitting wall remains unchanged (COLUMN)\n",
    "    if s_new_c == -1:\n",
    "        s_new_c = 0\n",
    "    elif s_new_c == 12:\n",
    "        s_new_c = 11\n",
    "        \n",
    "    # wind effect\n",
    "    if random.uniform(0,1)< wind_prob:\n",
    "        s_new_c = s_new_c + 1\n",
    "        \n",
    "    # after hitting wall remains unchanged (COLUMN)\n",
    "    if s_new_c == 12:\n",
    "        s_new_c = 11\n",
    "        \n",
    "    return s_new_r, s_new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    print(r)\n",
    "    #initial action value function\n",
    "    q_value = np.random.randint(0,10, (4, 12, 12)).astype(float)/10\n",
    "    q_value[:,goal_r,goal_c]=0\n",
    "    for ep in range(episodes):\n",
    "        # select one among the four start states\n",
    "        start_r=np.random.choice([5, 6, 10, 11])\n",
    "        start_c = 0 # first column\n",
    "        s_r = start_r\n",
    "        s_c = start_c\n",
    "        G = 0 \n",
    "        e=np.zeros([4,12,12])\n",
    "        if np.random.uniform(0,1)> epsilon_value:\n",
    "            act_sel=np.argmax(q_value[:,s_r, s_c])\n",
    "        else:\n",
    "            act_sel=np.random.choice([0,1, 2, 3])\n",
    "                \n",
    "            for k in range(M):\n",
    "                    \n",
    "                if s_r == goal_r and s_c == goal_c: \n",
    "                    break\n",
    "                else:\n",
    "                    [s_new_r,s_new_c] = statechangefn(act_sel,s_r, s_c)\n",
    "                    if np.random.uniform(0,1)> epsilon_value:\n",
    "                        act_next=np.argmax(q_value[:,s_new_r,s_new_c])\n",
    "                    else:\n",
    "                        act_next=np.random.choice([0,1, 2, 3])\n",
    "                        \n",
    "                    delta= grid_rewards[s_new_r, s_new_c]  + gamma*(q_value[act_next,s_new_r, s_new_c]) - q_value[act_sel,s_r, s_c]\n",
    "                    e[act_sel,s_r, s_c]=e[act_sel,s_r, s_c]+1\n",
    "                    for i in range(0,12):\n",
    "                        for j in range(0,12):\n",
    "                            for l in range(0,4):\n",
    "                                q_value[l,i,j] = q_value[l,i,j] + alpha*delta*e[l,i,j]\n",
    "                                e[l,i,j]=gamma*lmbda*e[l,i,j]\n",
    "                    s_r = s_new_r\n",
    "                    s_c = s_new_c\n",
    "                    act_sel=act_next\n",
    "                    G = G + gamma**(k)*grid_rewards[s_new_r,s_new_c]\n",
    "            steps[r,ep]=k\n",
    "            G_all [r,ep] = G\n",
    "avg_steps = np.mean(steps,0)\n",
    "avg_G = np.mean(G_all,0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_G,color=\"green\")\n",
    "plt.title('SARSA (lambda=0.9)Goal:C', fontsize=16)\n",
    "plt.xlabel('No of Episodes')\n",
    "plt.ylabel('Cummulative Discounted Reward')\n",
    "plt.savefig('C:/Users/Venkatachalam S/Desktop/POLICY/SLGA5.jpg',bbox_inches='tight',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_steps)\n",
    "plt.title('SARSA (lambda=0.9)Goal:C', fontsize=16)\n",
    "plt.xlabel('No of Episodes')\n",
    "plt.ylabel('Cummulative Discounted Reward')\n",
    "plt.savefig('C:/Users/Venkatachalam S/Desktop/POLICY/SLGA6.jpg',bbox_inches='tight',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
