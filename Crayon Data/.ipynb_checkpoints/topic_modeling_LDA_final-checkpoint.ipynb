{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "import string \n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\\\Consultancy\\\\Crayon\")\n",
    "df = DataFrame.from_csv(\"100K_Restaurants_reviews.tsv\", sep=\"\\t\")\n",
    "df['POS'] = pos_tag_sents(df['ReviewTxt'].apply(word_tokenize).tolist())\n",
    "df['noun_words'] = np.nan\n",
    "df['noun_words_sentec'] = np.nan\n",
    "df = df.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(df)):\n",
    "    tagged = df.iloc[j,9]\n",
    "    noun_words = []\n",
    "    for item in tagged:\n",
    "        if item[1] == 'N' or item[1] == 'NN' or item[1] == 'NNP' or item[1] == 'NNS' or item[1] == 'NNPS':\n",
    "            noun_words.append(item[0])\n",
    "    df.iloc[j,10] = noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99999x156 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 877463 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(df)):\n",
    "    temp = ' '.join(word for word in df.iloc[j,10])\n",
    "    df.iloc[j,11] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000 #top 1000 features are selected\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=0.02,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df['noun_words_sentec'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1742709.36275\n",
      "Model Perplexity:  97.2563100719\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic distribution over the review text\n",
    "Topic_distribution = DataFrame(data=lda_output)\n",
    "Topic_distribution.head()\n",
    "Topic_distribution.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\Topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# View\n",
    "df_topic_keywords.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\Topic_words_value.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose the best parameter based on this. It should be maximum\n",
    "# Get Log Likelyhoods from Grid Search Output\n",
    "\n",
    "n_topics = [5, 10, 15, 20, 30]\n",
    "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.savefig('E:\\\\Consultancy\\\\Crayon\\\\comparision.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=tf_vectorizer, lda_model=lda_model, n_words=100):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=best_lda_model, n_words=100)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\Topic_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  df['RestaurantId'].to_frame()\n",
    "data.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\ddummy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Final_topics = pd.DataFrame.from_csv(\"Topic_distribution.csv\")\n",
    "Restaurant = Final_topics.groupby(['RestaurantId']).mean()\n",
    "Restaurant.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\Restaurant_topics_distribution.csv\")\n",
    "Review = Final_topics.groupby(['ReviewId']).mean()\n",
    "Review.to_csv(\"E:\\\\Consultancy\\\\Crayon\\\\User_topics_distribution.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
